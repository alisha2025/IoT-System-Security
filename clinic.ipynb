{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHUa_0TdH0nd",
        "outputId": "53b26c5e-da85-4655-e686-800489c3faf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Now, you can use the path to your Google Drive like this\n",
        "prefix = '/content/drive/MyDrive/DS-MQTT/'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZxyZvfiw_YG",
        "outputId": "22691233-c8e9-43a4-891c-bc7a0a59207c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiRAOyQnIxXw",
        "outputId": "605b9a5a-88d2-4af7-8b07-c979b5ef32e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MQTT_ML'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 29 (delta 7), reused 9 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (29/29), 16.29 KiB | 5.43 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AbertayMachineLearningGroup/MQTT_ML.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD9YB3-AJEAE",
        "outputId": "da42149f-48df-4b72-8367-1d64f077cb85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MQTT_ML/notebooks'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd MQTT_ML/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9a69gTwJb1D",
        "outputId": "0a8e142f-98c0-41d1-8b2b-426b57021d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'MQTT_ML/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd MQTT_ML/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd9WoxLHKOvM"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pua0DCqGtJMr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Now, you can use the path to your Google Drive like this\n",
        "prefix = '/content/drive/MyDrive/DS-MQTT/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-bGxiXXEAqC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvN3ayVEFJT4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2YrL2myGRLX"
      },
      "outputs": [],
      "source": [
        "# helper function: to convert string data to boolean\n",
        "\n",
        "def str2bool(v):\n",
        "  if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "    return True\n",
        "  elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "    return False\n",
        "  else:\n",
        "    raise argparse.ArgumentTypeError('Booloean value expected.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE-JVenyKL2H"
      },
      "outputs": [],
      "source": [
        "# data preprocessing\n",
        "\n",
        "one_hot_encoder = None\n",
        "\n",
        "def load_file(path, mode, is_attack = 1, label = 1, folder_name='Bi/', sliceno = 0, verbose = True):\n",
        "    #global label_encoder\n",
        "    global one_hot_encoder\n",
        "\n",
        "    #attacker_ips = ['192.168.2.5']\n",
        "\n",
        "    columns_to_drop_packet = ['timestamp', 'src_ip', 'dst_ip']\n",
        "    columns_to_drop_uni = ['proto', 'ip_src', 'ip_dst']\n",
        "    columns_to_drop_bi = ['proto', 'ip_src', 'ip_dst']\n",
        "\n",
        "    if os.path.getsize(path)//10 ** 9 > 0:\n",
        "        x = np.zeros((0,0))\n",
        "        for chunk in pd.read_csv(path, chunksize=10 ** 6):\n",
        "            chunk.drop(columns = columns_to_drop_packet, inplace = True)\n",
        "            chunk = chunk[chunk.columns.drop(list(chunk.filter(regex='mqtt')))]\n",
        "\n",
        "            chunk = chunk.fillna(-1)\n",
        "\n",
        "            with open(folder_name + 'instances_count.csv','a') as f:\n",
        "                f.write('{}, {} \\n'.format(path, chunk.shape[0]))\n",
        "\n",
        "            x_temp = chunk.loc[chunk['is_attack'] == is_attack]\n",
        "            x_temp.drop('is_attack', axis = 1, inplace = True)\n",
        "            #x_temp['protocol'] = label_encoder.transform(x_temp['protocol'])\n",
        "            if one_hot_encoder == None:\n",
        "                one_hot_encoder = OneHotEncoder(categorical_features=[0], n_values=30)\n",
        "                x_temp = one_hot_encoder.fit_transform(x_temp).toarray()\n",
        "            else:\n",
        "                x_temp = one_hot_encoder.transform(x_temp).toarray()\n",
        "\n",
        "            x_temp = np.unique(x_temp, axis = 0)\n",
        "\n",
        "            if x.size == 0:\n",
        "                x = x_temp\n",
        "            else:\n",
        "                x = np.concatenate((x, x_temp), axis = 0)\n",
        "                x = np.unique(x, axis = 0)\n",
        "    else:\n",
        "        dataset = pd.read_csv(path)\n",
        "\n",
        "        if mode == 1 or mode == 2:\n",
        "            dataset = dataset.loc[dataset['is_attack'] == is_attack]\n",
        "#            if is_attack == 0:\n",
        "#                dataset = dataset.loc[operator.and_(dataset['ip_src'].isin(attacker_ips) == False, dataset['ip_dst'].isin(attacker_ips) == False)]\n",
        "#            else:\n",
        "#                dataset = dataset.loc[operator.or_(dataset['ip_src'].isin(attacker_ips), dataset['ip_dst'].isin(attacker_ips))]\n",
        "#\n",
        "        if mode == 0:\n",
        "            dataset.drop(columns=[columns_to_drop_packet], inplace = True)\n",
        "            dataset = dataset[dataset.columns.drop(list(dataset.filter(regex='mqtt')))]\n",
        "        elif mode == 1:\n",
        "            dataset.drop(columns = columns_to_drop_uni, inplace = True)\n",
        "        elif mode == 2:\n",
        "            dataset.drop(columns = columns_to_drop_bi, inplace = True)\n",
        "\n",
        "        if verbose:\n",
        "            print(dataset.columns)\n",
        "\n",
        "        dataset = dataset.fillna(-1)\n",
        "\n",
        "        if mode == 0:\n",
        "            x = dataset.loc[dataset['is_attack'] == is_attack]\n",
        "            x.drop('is_attack', axis=1, inplace=True)\n",
        "            #x['protocol'] = label_encoder.transform(x['protocol'])\n",
        "            if one_hot_encoder == None:\n",
        "                one_hot_encoder = OneHotEncoder(categorical_features=[0], n_values=30)\n",
        "                x = one_hot_encoder.fit_transform(x).toarray()\n",
        "            else:\n",
        "                x = one_hot_encoder.transform(x).toarray()\n",
        "        else:\n",
        "            x = dataset.values\n",
        "\n",
        "    with open(folder_name + 'instances_count.csv','a') as f:\n",
        "        f.write('all, {}, {} \\n'.format(path, x.shape[0]))\n",
        "\n",
        "    x = np.unique(x, axis = 0)\n",
        "\n",
        "    with open(folder_name + 'instances_count.csv','a') as f:\n",
        "        f.write('unique, {}, {} \\n'.format(path, x.shape[0]))\n",
        "\n",
        "    if (mode == 1 and x.shape[0] > 100000) or (mode == 2 and x.shape[0] > 50000):\n",
        "            temp = x.shape[0] // 10\n",
        "            start = sliceno * temp\n",
        "            end = start + temp - 1\n",
        "            x = x[start:end,:]\n",
        "            with open(folder_name + 'instances_count.csv','a') as f:\n",
        "                f.write('Start, {}, End, {} \\n'.format(start, end))\n",
        "    elif mode == 0:\n",
        "        if x.shape[0] > 15000000:\n",
        "            temp = x.shape[0] // 400\n",
        "            start = sliceno * temp\n",
        "            end = start + temp - 1\n",
        "            x = x[start:end,:]\n",
        "            with open(folder_name + 'instances_count.csv','a') as f:\n",
        "                f.write('Start, {}, End, {} \\n'.format(start, end))\n",
        "        elif x.shape[0] > 10000000:\n",
        "            temp = x.shape[0] // 200\n",
        "            start = sliceno * temp\n",
        "            end = start + temp - 1\n",
        "            x = x[start:end,:]\n",
        "            with open(folder_name + 'instances_count.csv','a') as f:\n",
        "                f.write('Start, {}, End, {} \\n'.format(start, end))\n",
        "        elif x.shape[0] > 100000:\n",
        "            temp = x.shape[0] // 10\n",
        "            start = sliceno * temp\n",
        "            end = start + temp - 1\n",
        "            x = x[start:end,:]\n",
        "            with open(folder_name + 'instances_count.csv','a') as f:\n",
        "                f.write('Start, {}, End, {} \\n'.format(start, end))\n",
        "\n",
        "\n",
        "    y = np.full(x.shape[0], label)\n",
        "\n",
        "    with open(folder_name + 'instances_count.csv','a') as f:\n",
        "        f.write('slice, {}, {} \\n'.format(path, x.shape[0]))\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09UaghUwsjRh"
      },
      "outputs": [],
      "source": [
        "#training\n",
        "\n",
        "def classify_sub(classifier, x_train, y_train, x_test, y_test, cm_file_name, summary_file_name, classifier_name, verbose = True):\n",
        "  classifier.fit(x_train, y_train)\n",
        "  pred = classifier.predict(x_test)\n",
        "\n",
        "  cm = pd.crosstab(y_test, pred)\n",
        "  cm.to_csv(cm_file_name)\n",
        "\n",
        "  pd.DataFrame(classification_report(y_test, pred, output_dict = True)).transpose().to_csv(summary_file_name)\n",
        "\n",
        "  if verbose:\n",
        "      print(classifier_name + ' Done.\\n')\n",
        "\n",
        "  del classifier\n",
        "  del pred\n",
        "  del cm\n",
        "\n",
        "def classify(random_state, x_train, y_train, x_test, y_test, folder_name, prefix = \"\", verbose = True):\n",
        "    confusion_matrix_folder = os.path.join(folder_name, 'Confusion_Matrix/')\n",
        "    summary_folder =  os.path.join(folder_name, 'Summary/')\n",
        "\n",
        "    if os.path.isdir(confusion_matrix_folder) == False:\n",
        "            os.mkdir(confusion_matrix_folder)\n",
        "    if os.path.isdir(summary_folder) == False:\n",
        "            os.mkdir(summary_folder)\n",
        "\n",
        "    # MLP\n",
        "    def classify_mlp(random_state, x_train, y_train, x_test, y_test, folder_name, prefix=\"\", verbose=True):\n",
        "        confusion_matrix_folder = os.path.join(folder_name, 'Confusion_Matrix/')\n",
        "        summary_folder = os.path.join(folder_name, 'Summary/')\n",
        "\n",
        "        if os.path.isdir(confusion_matrix_folder) == False:\n",
        "                os.mkdir(confusion_matrix_folder)\n",
        "        if os.path.isdir(summary_folder) == False:\n",
        "                os.mkdir(summary_folder)\n",
        "\n",
        "\n",
        "\n",
        "    # # 1- Linear\n",
        "    linear_classifier = LogisticRegression(random_state = random_state)\n",
        "    classify_sub(linear_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_linear.csv',\n",
        "                 summary_folder + prefix + '_summary_linear.csv',\n",
        "                 'Linear',\n",
        "                 verbose)\n",
        "\n",
        "    # 2- KNN\n",
        "    knn_classifier = KNeighborsClassifier()\n",
        "    classify_sub(knn_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_knn.csv',\n",
        "                 summary_folder + prefix + '_summary_knn.csv',\n",
        "                 'KNN',\n",
        "                 verbose)\n",
        "\n",
        "    #3- RBF SVM\n",
        "    kernel_svm_classifier = SVC(kernel = 'rbf', random_state = random_state, gamma='scale')\n",
        "    classify_sub(kernel_svm_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_kernel_svm.csv',\n",
        "                 summary_folder + prefix + '_summary_kernel_svm.csv',\n",
        "                 'SVM',\n",
        "                 verbose)\n",
        "\n",
        "    #4- Naive Bayes\n",
        "    naive_classifier = GaussianNB()\n",
        "    classify_sub(naive_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_naive.csv',\n",
        "                 summary_folder + prefix + '_summary_naive.csv',\n",
        "                 'Naive',\n",
        "                 verbose)\n",
        "\n",
        "    #5- Decision Tree\n",
        "    decision_tree_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = random_state)\n",
        "    classify_sub(decision_tree_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_decision_tree.csv',\n",
        "                 summary_folder + prefix + '_summary_decision_tree.csv',\n",
        "                 'Decision Tree',\n",
        "                 verbose)\n",
        "\n",
        "    #6- Random Forest\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = random_state)\n",
        "    classify_sub(random_forest_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_random_forest.csv',\n",
        "                 summary_folder + prefix + '_summary_random_forest.csv',\n",
        "                 'Random Forest',\n",
        "                 verbose)\n",
        "\n",
        "    # 7- Linear SVM\n",
        "    svm_classifier = LinearSVC(random_state = random_state)\n",
        "    classify_sub(svm_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_svm.csv',\n",
        "                 summary_folder + prefix + '_summary_svm.csv',\n",
        "                 'SVM',\n",
        "                 verbose)\n",
        "\n",
        "      # 8- Neural Network\n",
        "    mlp_classifier = MLPClassifier(hidden_layer_sizes=([100, 100, 100]), alpha=0.001)\n",
        "    classify_sub(mlp_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_mlp.csv',\n",
        "                 summary_folder + prefix + '_summary_mlp.csv',\n",
        "                 'MLP',\n",
        "                 verbose)\n",
        "\n",
        "    xgb_classifier = XGBClassifier(learning_rate=0.4, max_depth=7)\n",
        "    classify_sub(xgb_classifier,\n",
        "                 x_train, y_train,\n",
        "                 x_test, y_test,\n",
        "                 confusion_matrix_folder + prefix + '_cm_xbg.csv',\n",
        "                 summary_folder + prefix + '_summary_xbg.csv',\n",
        "                 'XBG',\n",
        "                 verbose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "JVJt72culD_o",
        "outputId": "1c9d1bc9-519c-41a3-ae9d-1a8e1297468d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Slice #: 0\n",
            "Start Classification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-983d24d6d27a>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  x_temp.drop('is_attack', axis = 1, inplace = True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-97da3fe0911d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         x, y = load_file('/content/drive/MyDrive/DS-MQTT/packet_features/mqtt_bruteforce.csv',\n\u001b[0m\u001b[1;32m     28\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                          \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-983d24d6d27a>\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(path, mode, is_attack, label, folder_name, sliceno, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m#x_temp['protocol'] = label_encoder.transform(x_temp['protocol'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mone_hot_encoder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mone_hot_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mx_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'categorical_features'"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--mode', type = int, default = 2)\n",
        "    parser.add_argument('--output', default='Classification_Bi')\n",
        "    parser.add_argument('--verbose', type = str2bool, default = True)\n",
        "    parser.add_argument(\"-f\", \"--file\", required=False)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    for slice_number in range(11):\n",
        "        prefix = ''\n",
        "        if args.mode == 1:\n",
        "            prefix = 'uniflow_'\n",
        "        elif args.mode == 2:\n",
        "            prefix = 'biflow_'\n",
        "\n",
        "        if args.verbose:\n",
        "            print('Starting Slice #: {}'.format(slice_number))\n",
        "            print('Start Classification')\n",
        "\n",
        "        random_state = 0\n",
        "        folder_name = '{}_{}/'.format(args.output, slice_number)\n",
        "\n",
        "        if os.path.isdir(folder_name) == False:\n",
        "            os.mkdir(folder_name)\n",
        "\n",
        "        x, y = load_file('/content/drive/MyDrive/DS-MQTT/packet_features/mqtt_bruteforce.csv',\n",
        "                         args.mode,\n",
        "                         0, 0,\n",
        "                         folder_name,\n",
        "                         slice_number,\n",
        "                         args.verbose)\n",
        "\n",
        "\n",
        "        x_temp, y_temp = load_file('/content/drive/MyDrive/DS-MQTT/biflow_features/biflow_scan_A.csv',\n",
        "                                   args.mode,\n",
        "                                   1, 1,\n",
        "                                   folder_name,\n",
        "                                   slice_number,\n",
        "                                   args.verbose)\n",
        "\n",
        "        x = np.concatenate((x, x_temp), axis = 0)\n",
        "        y = np.append(y, y_temp)\n",
        "        del x_temp, y_temp\n",
        "\n",
        "        x_temp, y_temp = load_file('/content/drive/MyDrive/DS-MQTT/biflow_features/biflow_scan_sU.csv',\n",
        "                                   args.mode,\n",
        "                                   1, 2,\n",
        "                                   folder_name,\n",
        "                                   slice_number,\n",
        "                                   args.verbose)\n",
        "\n",
        "        x = np.concatenate((x, x_temp), axis = 0)\n",
        "        y = np.append(y, y_temp)\n",
        "        del x_temp, y_temp\n",
        "\n",
        "        x_temp, y_temp = load_file('/content/drive/MyDrive/DS-MQTT/biflow_features/biflow_sparta.csv',\n",
        "                                   args.mode,\n",
        "                                   1, 3,\n",
        "                                   folder_name,\n",
        "                                   slice_number,\n",
        "                                   args.verbose)\n",
        "\n",
        "        x = np.concatenate((x, x_temp), axis = 0)\n",
        "        y = np.append(y, y_temp)\n",
        "        del x_temp, y_temp\n",
        "\n",
        "        x_temp, y_temp = load_file('/content/drive/MyDrive/DS-MQTT/biflow_features/biflow_mqtt_bruteforce.csv',\n",
        "                                   args.mode,\n",
        "                                   1, 4,\n",
        "                                   folder_name,\n",
        "                                   slice_number,\n",
        "                                   args.verbose)\n",
        "\n",
        "        x = np.concatenate((x, x_temp), axis = 0)\n",
        "        y = np.append(y, y_temp)\n",
        "        del x_temp, y_temp\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
        "                                                            test_size = 0.25,\n",
        "                                                            random_state = 42)\n",
        "\n",
        "        classify(random_state, x_train, y_train, x_test, y_test,\n",
        "                 folder_name, \"slice_{}_no_cross_validation\".format(slice_number), args.verbose)\n",
        "\n",
        "        kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n",
        "\n",
        "        counter = 0\n",
        "        for train, test in kfold.split(x, y):\n",
        "            classify(random_state, x[train], y[train], x[test], y[test],\n",
        "                     folder_name, \"slice_{}_k_{}\".format(slice_number, counter), args.verbose)\n",
        "            counter += 1\n",
        "\n",
        "        del x\n",
        "        del y\n",
        "        del x_train\n",
        "        del x_test\n",
        "        del y_train\n",
        "        del y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETkgh9dRucGs",
        "outputId": "e39e49bf-4b11-4cc8-f28c-c938b60de630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prefix: biflow_\n",
            "Full File Path: biflow_normal.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"Prefix:\", prefix)\n",
        "file_path = prefix + 'normal.csv'\n",
        "print(\"Full File Path:\", file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcxdqV6tuFc6",
        "outputId": "4457e884-2c13-49c2-ee2a-8d1c1daf367b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File does not exist\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists('biflow_scan_A.csv'):\n",
        "    print('File exists')\n",
        "else:\n",
        "    print('File does not exist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HNryXPAVXpR",
        "outputId": "7d0f43ff-ea18-4ec6-bd07-95ce5254dd4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/Classification_Bi_9 /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "cIkSpo40BwbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}